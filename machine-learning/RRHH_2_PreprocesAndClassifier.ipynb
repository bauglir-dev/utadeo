{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RRHH_2_PreprocesAndClassifier.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utl8LOA-dXCh"
      },
      "source": [
        "**1. Load Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9a8qpOsibWEG"
      },
      "source": [
        "#Pandas: Dataset library\n",
        "import pandas as pd\n",
        "\n",
        "#Read dataset\n",
        "rrhh_ds = pd.read_csv(\"https://docs.google.com/spreadsheets/d/e/2PACX-1vSfE_9K1uPYtXkEA_oncBZEGne6D6uhJSBCDqASxid4jIlQiyu_Z3629s0yyhNQxxkb6Q4wnUwDtlNr/pub?gid=0&single=true&output=csv\")\n",
        "\n",
        "#Print dataset\n",
        "rrhh_ds.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGOtq8UTdq4p"
      },
      "source": [
        "**2. Filter Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttPsk5amdWQk"
      },
      "source": [
        "#Remove some attributes\n",
        "rrhh_ds = rrhh_ds[['NivelSatisfaccion', 'UltimaEvaluacion', 'ProyectosRealizados', 'HorasMensuales', 'Antiguedad', 'Ascendido_Disc', 'AreaTrabajo', 'NivelSalarial', 'Renuncia_Disc']]\n",
        "\n",
        "#Print dataset\n",
        "rrhh_ds.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YT7qskwue1vj"
      },
      "source": [
        "**3.1. Preprocess Dataset Part 1: Standard Scaler**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTKHRKFRe5bv"
      },
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "#Obtain Standard Transformation\n",
        "std_scaler = preprocessing.StandardScaler().fit(rrhh_ds[['ProyectosRealizados']])\n",
        "\n",
        "#Check results of transformation\n",
        "print(\"\\n=====Before Standard Scaler:=====\\n\")\n",
        "print(\"Mean:\",std_scaler.mean_)\n",
        "print(\"Std:\",std_scaler.scale_)\n",
        "\n",
        "#Apply the transformation (create temporal variable, do not transform original dataset)\n",
        "rrhh_ds_pr_temp = std_scaler.transform(rrhh_ds[['ProyectosRealizados']])\n",
        "\n",
        "#Obtain Standard Transformation\n",
        "std_scaler = preprocessing.StandardScaler().fit(rrhh_ds_pr_temp)\n",
        "\n",
        "#Check results of transformation\n",
        "print(\"\\n=====After Standard Scaler:=====\\n\")\n",
        "print(\"Mean:\",std_scaler.mean_)\n",
        "print(\"Std:\",std_scaler.scale_)\n",
        "\n",
        "#Print dataset\n",
        "#rrhh_ds.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flw6iGQzk5su"
      },
      "source": [
        "**3.2. Preprocess Dataset - Part 2: MinMax Scaler**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMA4wZy2j5G3"
      },
      "source": [
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "\n",
        "#Transforma data to [0-1] scale. \n",
        "rrhh_ds[['ProyectosRealizados', 'HorasMensuales', 'Antiguedad' ]] = min_max_scaler.fit_transform(rrhh_ds[['ProyectosRealizados', 'HorasMensuales', 'Antiguedad']])\n",
        "\n",
        "#Print dataset\n",
        "rrhh_ds.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXfZ82fAvfy2"
      },
      "source": [
        "**3.3. Preprocess Dataset - Part 3: One Hot Encoder**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWxcRBEmvjB7"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "#Creating instance of one-hot-encoder\n",
        "enc = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "#Passing columns to encoding\n",
        "enc_df = pd.DataFrame(enc.fit_transform(rrhh_ds[['AreaTrabajo', 'NivelSalarial']]).toarray())\n",
        "\n",
        "#Merge with main rrhh_ds on key values\n",
        "rrhh_ds = rrhh_ds.join(enc_df)\n",
        "\n",
        "#Drop old columns\n",
        "rrhh_ds = rrhh_ds.drop(['AreaTrabajo', 'NivelSalarial'],axis=1)\n",
        "rrhh_ds\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSw9Klbp6saw"
      },
      "source": [
        "**3.4. Preprocess Dataset - Part 4: Label Encoder**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rj0abfmm4FnU"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "#Transform YES/NO to 0/1\n",
        "lb = LabelEncoder() \n",
        "rrhh_ds['Ascendido_Disc'] = lb.fit_transform(rrhh_ds['Ascendido_Disc'])\n",
        "rrhh_ds['Renuncia_Disc'] = lb.fit_transform(rrhh_ds['Renuncia_Disc'])\n",
        "\n",
        "rrhh_ds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYPuwbNOnyGf"
      },
      "source": [
        "**4A. Feature Selection: Wrapper methods**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-OQPx2GnbMl"
      },
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "#Split X and Y\n",
        "Y = rrhh_ds['Renuncia_Disc']\n",
        "X = rrhh_ds.drop(['Renuncia_Disc'], axis=1)\n",
        "\n",
        "print(\"RRHH Dataset size: \", X.shape)\n",
        "\n",
        "clf = ExtraTreesClassifier(n_estimators=50)\n",
        "\n",
        "clf = clf.fit(X, Y)\n",
        "\n",
        "print(clf.feature_importances_ )\n",
        "\n",
        "model = SelectFromModel(clf, prefit=True)\n",
        "\n",
        "X_new = model.transform(X)\n",
        "\n",
        "print(\"X_new Dataset size: \", X_new.shape)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGAyGdkkp8WE"
      },
      "source": [
        "**4B. Feature Selection: Embedded methods**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3STPwewjaR-"
      },
      "source": [
        "from sklearn.linear_model import Lasso, LogisticRegression\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "import numpy as np\n",
        "\n",
        "sel_ = SelectFromModel(LogisticRegression(C=1, penalty='l1', solver='liblinear'))\n",
        "\n",
        "sel_.fit(X, Y)\n",
        "\n",
        "selected_feat = X.columns[(sel_.get_support())]\n",
        "\n",
        "print('\\nTotal features: {}'.format((X.shape[1])))\n",
        "print('\\nSelected features: {}'.format(len(selected_feat)))\n",
        "print('\\nFeatures with coefficients shrank to zero: {}'.format(np.sum(sel_.estimator_.coef_ == 0)))\n",
        "\n",
        "removed_feats = X.columns[(sel_.estimator_.coef_ == 0).ravel().tolist()]\n",
        "\n",
        "print(\"\\nRemove features:\\n\")\n",
        "print(removed_feats)\n",
        "\n",
        "#Remove features and build a new filter dataset\n",
        "X_new = sel_.transform(X.fillna(0))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHOQmp3NqNV6"
      },
      "source": [
        "**4C. Feature Selection: Filter methods**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRIgQlDmnxeZ"
      },
      "source": [
        "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
        "\n",
        "NUMBER_FEATURES = 10\n",
        "\n",
        "#Split X and Y\n",
        "Y = rrhh_ds['Renuncia_Disc']\n",
        "X = rrhh_ds.drop(['Renuncia_Disc'], axis=1)\n",
        "\n",
        "selector = SelectKBest(mutual_info_classif, k=NUMBER_FEATURES)\n",
        "selector.fit(X, Y)\n",
        "\n",
        "#Set X_new column names\n",
        "cols = selector.get_support(indices=True)\n",
        "X_new = X.iloc[:,cols]\n",
        "\n",
        "#Set y column names\n",
        "Y = pd.DataFrame(Y)\n",
        "Y.columns = ['Renuncia']\n",
        "\n",
        "print(\"\\nSelected Attributes:\\n\")\n",
        "print(X_new[:NUMBER_FEATURES])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fxdbzk3_1eEd"
      },
      "source": [
        "**5. Split Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvCx2pwl1dZA"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "rrhh_ds = X_new.join(Y)\n",
        "\n",
        "#Split dataset (train and test)\n",
        "train, test = train_test_split(rrhh_ds, test_size=0.15)\n",
        "\n",
        "#Split X e Y\n",
        "X_train = train.drop('Renuncia',axis=1)\n",
        "X_test = test.drop('Renuncia',axis=1)\n",
        "y_train = train['Renuncia']\n",
        "y_test = test['Renuncia']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNBCDOBA28iC"
      },
      "source": [
        "**6. Logistic Regression**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ombITFTL2WwP"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "clf = LogisticRegression(random_state=0,penalty='l2').fit(X_train, y_train)\n",
        "\n",
        "#Evaluate Model\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "\n",
        "predictions = clf.predict(X_test)\n",
        "\n",
        "print(\"\\n=====Confusion Matrix (Logistic Regression):=====\\n\")\n",
        "print(confusion_matrix(y_test,predictions))\n",
        "\n",
        "print(\"\\n=====Classification Report (Logistic Regression):=====\\n\")\n",
        "print(classification_report(y_test,predictions))\n",
        "\n",
        "#Plot ROC Curve\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fpr, tpr, _ = metrics.roc_curve(y_test,  predictions)\n",
        "auc = metrics.roc_auc_score(y_test, predictions)\n",
        "plt.plot(fpr,tpr,label=\"ROC, auc=\"+str(auc))\n",
        "plt.legend(loc=4)\n",
        "\n",
        "print(\"\\n=====ROC Curve:=====\\n\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPCOMFfc7Oi0"
      },
      "source": [
        "**7. Decision Trees**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDipOspp5fci"
      },
      "source": [
        "from sklearn import tree\n",
        "\n",
        "clf_dt = tree.DecisionTreeClassifier()\n",
        "clf_dt = clf_dt.fit(X_train, y_train)\n",
        "\n",
        "#Evaluate Model\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "\n",
        "predictions = clf_dt.predict(X_test)\n",
        "\n",
        "print(\"\\n=====Confusion Matrix (Decision Tree):=====\\n\")\n",
        "print(confusion_matrix(y_test,predictions))\n",
        "\n",
        "print(\"\\n=====Classification Report (Decision Tree):=====\\n\")\n",
        "print(classification_report(y_test,predictions))\n",
        "\n",
        "#Plot ROC Curve\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fpr, tpr, _ = metrics.roc_curve(y_test,  predictions)\n",
        "auc = metrics.roc_auc_score(y_test, predictions)\n",
        "plt.plot(fpr,tpr,label=\"ROC, auc=\"+str(auc))\n",
        "plt.legend(loc=4)\n",
        "\n",
        "print(\"\\n=====ROC Curve:=====\\n\")\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQz8gqKWxCNK"
      },
      "source": [
        "**8. Random Forrest**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrsWwj16xBbE"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "clf_rf = RandomForestClassifier(max_depth=5, random_state=0)\n",
        "clf_rf = clf_rf.fit(X_train, y_train)\n",
        "\n",
        "#Evaluate Model\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "\n",
        "predictions = clf_rf.predict(X_test)\n",
        "\n",
        "print(\"\\n=====Confusion Matrix (Random Forrest):=====\\n\")\n",
        "print(confusion_matrix(y_test,predictions))\n",
        "\n",
        "print(\"\\n=====Classification Report (Random Forrest):=====\\n\")\n",
        "print(classification_report(y_test,predictions))\n",
        "\n",
        "#Plot ROC Curve\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fpr, tpr, _ = metrics.roc_curve(y_test,  predictions)\n",
        "auc = metrics.roc_auc_score(y_test, predictions)\n",
        "plt.plot(fpr,tpr,label=\"ROC, auc=\"+str(auc))\n",
        "plt.legend(loc=4)\n",
        "\n",
        "print(\"\\n=====ROC Curve:=====\\n\")\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}