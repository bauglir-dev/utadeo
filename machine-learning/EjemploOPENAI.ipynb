{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"EjemploOPENAI.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"KogLKIT734IT","colab_type":"text"},"source":["# Ejemplo Aprendizage por refuerzo"]},{"cell_type":"markdown","metadata":{"id":"37gvdSRewZsE","colab_type":"text"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"oC7mZrxT34Ia","colab_type":"text"},"source":["El siguiente ejemplo quiere mostrar un caso sencillo de aprendizage por refuerzo utilizando Python y la libreria de OPENAI Gym.\n","\n","El ejemplo trata de un taxi que necesita desplazar personas entre unos puntos (RGBY). El taxi puede realizar acciones como lo son desplazarse arriba, abajo, derecha, izquierda, recoger y dejar al pasajero. \n","\n","Cada movimiento resta un punto al taxi, por cada vez que se recoje o se deja a alguien en un sitio incorrecto se restan 10 puntos y por cada vez que un pasajero es dejado en su destino se suman 20 puntos."]},{"cell_type":"markdown","metadata":{"id":"dq-952l934Ii","colab_type":"text"},"source":["El siguiente codigo ha sido tomado de https://builtin.com/data-science/reinforcement-learning-python y https://gist.github.com/enzodeal/ae196380f80df9ff522d34f84f324e51. "]},{"cell_type":"markdown","metadata":{"id":"JrLrweNHrtTJ","colab_type":"text"},"source":["El codigo anterior busca simular de manera aleatoria el comportamiento del taxi y termina cuando el taxi deja por primera vez a un pasajero."]},{"cell_type":"code","metadata":{"id":"q1_Rbk5R34Io","colab_type":"code","outputId":"e1512622-db1b-4bcb-f550-0041e3fe382e","executionInfo":{"status":"error","timestamp":1587609778128,"user_tz":300,"elapsed":36020,"user":{"displayName":"Jhoan Sebastian Almeida Caicedo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiKkyFOEKzpR_zC8zOC8sbWMawQ_GqgJmrUA0jJWg=s64","userId":"03951227817753240835"}},"colab":{"base_uri":"https://localhost:8080/","height":571}},"source":["import gym #una de las bibliotecas más utilizadas para resolver problemas de aprendizaje por refuerzo\n","from IPython.display import clear_output\n","from time import sleep\n","\n","# Creating thr env\n","#Todos los modelos y la interfaz para este problema ya están configurados en Gym y nombrados bajo  Taxi-V3\n","#Env es el núcleo de OpenAi Gym, que es la interfaz de entorno unificado. Los siguientes son los métodos env que nos serán muy útiles:\n","\n","  #env.reset : restablece el entorno y devuelve un estado inicial aleatorio.\n","  #env.step (acción):  Paso del entorno por un paso de tiempo.\n","  #env.step (acción): Devuelve las siguientes variables:\n","    #observation: Observaciones del medio ambiente.\n","    #reward: Si su acción fue beneficiosa o no.\n","    #done: Indica si hemos recogido y dejado con éxito a un pasajero, también llamado un episodio.\n","    #info: Información adicional como el rendimiento y la latencia con fines de depuración.\n","    #env.render: Renderiza un cuadro del entorno (útil para visualizar el entorno).\n","    \n","\n","#Hay una recompensa de -1 por cada acción y una recompensa adicional de +20 por entregar al pasajero. Hay una recompensa de -10 por ejecutar acciones 'recoger' y 'dejar' ilegalmente.\n","\n","env = gym.make(\"Taxi-v3\").env\n","\n","print(env)\n","env.s = 328 #\n","\n","\n","# Setting the number of iterations, penalties and reward to zero,\n","epochs = 0\n","penalties, reward = 0, 0\n","\n","frames = []\n","\n","done = False\n","\n","while not done: #Se crea bucle infinito y termina cuando deja la persona que recogio\n","    action = env.action_space.sample()\n","    state, reward, done, info = env.step(action)\n","\n","    if reward == -10:\n","        penalties += 1\n","\n","    # Put each rendered frame into the dictionary for animation\n","    frames.append({\n","        'frame': env.render(mode='ansi'),\n","        'state': state,\n","        'action': action,\n","        'reward': reward\n","    }\n","    )\n","\n","    epochs += 1\n","\n","# Printing all the possible actions, states, rewards.\n","def show(x):\n","    for i, frame in enumerate(frames):\n","        clear_output(wait=True)\n","        print(frame['frame'])\n","        print(f\"Timestep: {i + 1}\")\n","        print(f\"State: {frame['state']}\")\n","        print(f\"Action: {frame['action']}\")\n","        print(f\"Reward: {frame['reward']}\")\n","        sleep(.1)\n","\n","        \n","show(frames)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["+---------+\n","|\u001b[35mR\u001b[0m: | : :G|\n","|\u001b[43m \u001b[0m: | : : |\n","| : : : : |\n","| | : | : |\n","|\u001b[34;1mY\u001b[0m| : |B: |\n","+---------+\n","  (North)\n","\n","Timestep: 334\n","State: 108\n","Action: 1\n","Reward: -1\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-52c7b0618144>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-1-52c7b0618144>\u001b[0m in \u001b[0;36mshow\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Action: {frame['action']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Reward: {frame['reward']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"d0Xgv_Tyslid","colab_type":"text"},"source":["El siguiente codigo intenta aplicar un algoritmo de Qlearn para optimizar la manera en que el taxi se encarga de recojer y dejar en sus destinos a los ususarios."]},{"cell_type":"code","metadata":{"id":"-Nok9iH_34JE","colab_type":"code","outputId":"79b44a1b-caa7-4492-ba7c-5d3f0b8d7146","executionInfo":{"status":"ok","timestamp":1587609908851,"user_tz":300,"elapsed":61468,"user":{"displayName":"Jhoan Sebastian Almeida Caicedo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiKkyFOEKzpR_zC8zOC8sbWMawQ_GqgJmrUA0jJWg=s64","userId":"03951227817753240835"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["import numpy as np\n","import random\n","\n","\n","# Init Taxi-V2 Env\n","env = gym.make(\"Taxi-v3\").env\n","\n","# Init arbitary values\n","q_table = np.zeros([env.observation_space.n, env.action_space.n])\n","\n","# Hyperparameters\n","alpha = 0.1\n","gamma = 0.6\n","epsilon = 0.1\n","\n","\n","all_epochs = []\n","all_penalties = []\n","\n","for i in range(1, 100001):\n","    state = env.reset()\n","\n","    # Init Vars\n","    epochs, penalties, reward, = 0, 0, 0\n","    done = False\n","\n","    while not done:\n","        if random.uniform(0, 1) < epsilon:\n","            # Check the action space\n","            action = env.action_space.sample()\n","        else:\n","            # Check the learned values\n","            action = np.argmax(q_table[state])\n","\n","        next_state, reward, done, info = env.step(action)\n","\n","        old_value = q_table[state, action]\n","        next_max = np.max(q_table[next_state])\n","\n","        # Update the new value\n","        new_value = (1 - alpha) * old_value + alpha * \\\n","            (reward + gamma * next_max)\n","        q_table[state, action] = new_value\n","\n","        if reward == -10:\n","            penalties += 1\n","\n","        state = next_state\n","        epochs += 1\n","\n","    if i % 100 == 0:\n","        clear_output(wait=True)\n","        print(\"Episode: {i}\")\n","\n","print(\"Training finished.\")"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Episode: {i}\n","Training finished.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"phPNz6C0syc9","colab_type":"text"},"source":["La siguiente matriz almacena los resultados del algoritmo anterior filas estados columnas acciones"]},{"cell_type":"code","metadata":{"id":"jNObn6-P34Jw","colab_type":"code","outputId":"8b09599b-9776-48f9-d8cf-15568e32ebb7","executionInfo":{"status":"ok","timestamp":1587609914852,"user_tz":300,"elapsed":541,"user":{"displayName":"Jhoan Sebastian Almeida Caicedo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiKkyFOEKzpR_zC8zOC8sbWMawQ_GqgJmrUA0jJWg=s64","userId":"03951227817753240835"}},"colab":{"base_uri":"https://localhost:8080/","height":407}},"source":["import pandas as pd\n","\n","results = pd.DataFrame(q_table)\n","results\n","\n"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>-2.418371</td>\n","      <td>-2.363951</td>\n","      <td>-2.418371</td>\n","      <td>-2.363951</td>\n","      <td>-2.273252</td>\n","      <td>-11.363951</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>-1.870144</td>\n","      <td>-1.450240</td>\n","      <td>-1.870144</td>\n","      <td>-1.450240</td>\n","      <td>-0.750400</td>\n","      <td>-10.450240</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>-2.363951</td>\n","      <td>-2.273252</td>\n","      <td>-2.363951</td>\n","      <td>-2.273252</td>\n","      <td>-2.122086</td>\n","      <td>-11.273252</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>-2.496192</td>\n","      <td>-2.496768</td>\n","      <td>-2.496192</td>\n","      <td>-2.496801</td>\n","      <td>-9.678722</td>\n","      <td>-9.492347</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>495</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>496</th>\n","      <td>-2.143547</td>\n","      <td>-2.122033</td>\n","      <td>-2.145673</td>\n","      <td>-2.122033</td>\n","      <td>-6.288920</td>\n","      <td>-7.639420</td>\n","    </tr>\n","    <tr>\n","      <th>497</th>\n","      <td>-1.041943</td>\n","      <td>0.416000</td>\n","      <td>-1.101089</td>\n","      <td>-1.230037</td>\n","      <td>-4.128253</td>\n","      <td>-4.131789</td>\n","    </tr>\n","    <tr>\n","      <th>498</th>\n","      <td>-2.181177</td>\n","      <td>-2.122005</td>\n","      <td>-2.158237</td>\n","      <td>-2.122011</td>\n","      <td>-5.734670</td>\n","      <td>-4.441863</td>\n","    </tr>\n","    <tr>\n","      <th>499</th>\n","      <td>3.962544</td>\n","      <td>1.090988</td>\n","      <td>3.766542</td>\n","      <td>11.000000</td>\n","      <td>-2.646856</td>\n","      <td>-2.366879</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>500 rows × 6 columns</p>\n","</div>"],"text/plain":["            0         1         2          3         4          5\n","0    0.000000  0.000000  0.000000   0.000000  0.000000   0.000000\n","1   -2.418371 -2.363951 -2.418371  -2.363951 -2.273252 -11.363951\n","2   -1.870144 -1.450240 -1.870144  -1.450240 -0.750400 -10.450240\n","3   -2.363951 -2.273252 -2.363951  -2.273252 -2.122086 -11.273252\n","4   -2.496192 -2.496768 -2.496192  -2.496801 -9.678722  -9.492347\n","..        ...       ...       ...        ...       ...        ...\n","495  0.000000  0.000000  0.000000   0.000000  0.000000   0.000000\n","496 -2.143547 -2.122033 -2.145673  -2.122033 -6.288920  -7.639420\n","497 -1.041943  0.416000 -1.101089  -1.230037 -4.128253  -4.131789\n","498 -2.181177 -2.122005 -2.158237  -2.122011 -5.734670  -4.441863\n","499  3.962544  1.090988  3.766542  11.000000 -2.646856  -2.366879\n","\n","[500 rows x 6 columns]"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"ChCUevsutdF9","colab_type":"text"},"source":["El siguiente codigo ejecuta el modelo que creamos anteriormente con la matriz q para demostrar como trabaja el modelo."]},{"cell_type":"code","metadata":{"id":"DVfIrD_8kdHa","colab_type":"code","outputId":"9b765d74-0d7a-42a5-adb0-6f1f2df242b0","executionInfo":{"status":"ok","timestamp":1587609944122,"user_tz":300,"elapsed":15137,"user":{"displayName":"Jhoan Sebastian Almeida Caicedo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiKkyFOEKzpR_zC8zOC8sbWMawQ_GqgJmrUA0jJWg=s64","userId":"03951227817753240835"}},"colab":{"base_uri":"https://localhost:8080/","height":217}},"source":["#probando algoritmo\n","total_epoch, total_penalties = 0, 0\n","episodes = 5\n","\n","for _ in range(episodes):\n","    state = env.reset()\n","    epochs, penalties, reward = 0, 0, 0\n","    \n","    finished = False\n","    \n","    while not finished:\n","        action = np.argmax(q_table[state])\n","        state, reward, finished, info = env.step(action)\n","\n","        if reward == -10:\n","            penalties += 1\n","\n","        epochs += 1\n","        \n","        clear_output(wait=True)\n","        env.render()\n","        sleep(.25)\n","\n","    total_penalties += penalties\n","    total_epoch += epochs\n","\n","print(f\"Results after {episodes} episodes:\")\n","print(f\"Mean steps: {total_epoch / episodes}\")\n","print(f\"Mean penalties: {total_penalties / episodes}\")"],"execution_count":4,"outputs":[{"output_type":"stream","text":["+---------+\n","|R: | : :G|\n","| : | : : |\n","| : : : : |\n","| | : | : |\n","|\u001b[35m\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m\u001b[0m| : |B: |\n","+---------+\n","  (Dropoff)\n","Results after 5 episodes:\n","Mean steps: 11.8\n","Mean penalties: 0.0\n"],"name":"stdout"}]}]}